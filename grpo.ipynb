{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ede07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GRPO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import re\n",
    "pattern = r'\\[.*?\\]'\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"/opt/tiger/DataX/train/GRPO-train-1112-for-qwen.jsonl\", split=\"train\")\n",
    "\n",
    "model_path = \"/mnt/bn/brench-lf-volume/wkq_wsp/checkpoint-1340\"\n",
    "# 加载训练好的奖励模型\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "reward_model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "\n",
    "system_prompt = \"\"\"你是评论质量评估专家,需评估以下维度并输出0-1分数(越高越好):\n",
    "1. 相关性:与内容关联紧密;\n",
    "2. 情绪:中性/正面优于负面;\n",
    "3. 观点:清晰有逻辑;\n",
    "4. 提问:具体有意义;\n",
    "5. 俏皮话:自然有趣。\"\"\"\n",
    "base_length: int = 12\n",
    "sigma: float = 3.0\n",
    "length_type: str = \"char\"\n",
    "penalty_coef: float = 0.1\n",
    "max_penalty = 100\n",
    "\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "\n",
    "import emoji\n",
    "def extract_emojis(text):\n",
    "    # 使用列表推导式遍历文本中的每个字符，判断是否为 Emoji\n",
    "    return [char for char in text if emoji.is_emoji(char)]\n",
    "\n",
    "def reward_with_length_penalty(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    带长度惩罚的奖励函数，支持多卡训练\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    origin_reward = []\n",
    "    cplx_texts = []\n",
    "    \n",
    "    # 获取当前进程的设备（多卡训练时每个进程有不同的local_rank）\n",
    "    if torch.distributed.is_initialized():\n",
    "        local_rank = torch.distributed.get_rank()\n",
    "        device = torch.device(f\"cuda:{local_rank}\")\n",
    "    else:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 确保reward_model在正确的设备上\n",
    "    if not hasattr(reward_with_length_penalty, '_model_moved'):\n",
    "        reward_model.to(device)\n",
    "        reward_with_length_penalty._model_moved = True\n",
    "    \n",
    "    for idx, completion in enumerate(completions):\n",
    "        # 1. 获取生成的回复文本\n",
    "        completion_text = completion[0][\"content\"].split(\"</think>\\n\\n\")[1]\n",
    "        cplx_texts.append(completion_text)\n",
    "\n",
    "        # 提取用户输入内容(健壮性处理)\n",
    "        try:\n",
    "            user_content = next(\n",
    "                x[\"content\"] for x in prompts[0] if x[\"role\"] == \"user\"\n",
    "            )\n",
    "            if not user_content:\n",
    "                raise ValueError(\"用户输入内容为空\")\n",
    "        except StopIteration:\n",
    "            raise ValueError(\"Prompt中未找到user角色的内容\")\n",
    "        \n",
    "        # 2. 计算生成文本的长度\n",
    "        if length_type == \"char\":\n",
    "            length = len(completion_text)\n",
    "        elif length_type == \"token\":\n",
    "            # Token数计算(排除特殊Token,如<|im_start|>)\n",
    "            length = len(tokenizer.encode(completion_text, add_special_tokens=False))\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的长度类型:{length_type},请选'char'或'token'\")\n",
    "        \n",
    "        # 3. 计算长度惩罚项(核心:负二次函数)\n",
    "        delta = abs(length - base_length)  # 偏离基准的差值(正=过长,负=过短)\n",
    "        penalty_term = 3 if delta == 0 else -penalty_coef * (delta ** 2)  # 惩罚项:负二次函数(偏离越大,越负)\n",
    "        \n",
    "        # 可选:限制最大惩罚(防止惩罚过度,如max_penalty=10→惩罚不超过扣10分)\n",
    "        if max_penalty is not None:\n",
    "            penalty_term = max(-max_penalty, penalty_term)  # 惩罚项≥-max_penalty\n",
    "\n",
    "        # 4. 构造模型输入(遵循ChatML格式)\n",
    "        input_text = (\n",
    "            f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n{user_content}<|im_end|>\"\n",
    "        )\n",
    "        # Tokenize输入(适配模型输入格式,自动Padding/Truncation)\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            completion_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=10000  # 根据模型最大序列长度调整\n",
    "        )\n",
    "        \n",
    "        # 关键修复:将inputs移动到与模型相同的设备上\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # 5. 模型评分(无梯度计算)\n",
    "        with torch.no_grad():\n",
    "            outputs = reward_model(**inputs)\n",
    "            # 注意:需根据模型输出调整(此处假设模型输出标量评分)\n",
    "            raw_score = outputs.logits.item()  # 原始得分(越高越好)\n",
    "\n",
    "\n",
    "        # 定义正则表达式模式，用于匹配表情标签\n",
    "        re.findall(pattern, completion_text)\n",
    "        format_reward = -3 if \"\\n\" in completion_text or \"。\" in completion_text else 1\n",
    "        format_reward += len(re.findall(pattern, completion_text)) * -2\n",
    "        emoji_len = len(extract_emojis(completion_text))\n",
    "        format_reward += (emoji_len * -2) if emoji_len > 0 else 1\n",
    "        \n",
    "        # 6. 最终奖励 = 原始得分 + 长度惩罚项(可负)\n",
    "        final_reward = raw_score + penalty_term + format_reward\n",
    "        origin_reward.append(raw_score)\n",
    "        rewards.append(final_reward)\n",
    "\n",
    "    print(f\"completion_text:{cplx_texts}\")\n",
    "    print(f\"原始奖励:{origin_reward}\")\n",
    "    print(f\"带长度惩罚的奖励:{rewards}\")\n",
    "    return rewards\n",
    "\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"/mnt/bn/brench-lf-volume/wkq_wsp/train_result/Qwen3-4B-SFT-GRPO-1113\",\n",
    "    max_prompt_length=10000,\n",
    "    epsilon_high=0.28,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"tensorboard\",\n",
    "    beta=0.001,\n",
    "    logging_steps=2,\n",
    "    # 可选优化配置\n",
    "    gradient_checkpointing=True,  # 节省显存\n",
    "    bf16=True,  # 使用BF16混合精度\n",
    "    temperature=1,\n",
    "    top_p=1\n",
    ")\n",
    "\n",
    "model_name_or_path = \"/mnt/bn/brench-lf-volume/wkq_wsp/train_result/Qwen3-4B-Inst-SFT-1113/checkpoint-82\"\n",
    "# 2. 加载tokenizer\n",
    "# train_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"left\")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model_name_or_path,\n",
    "    reward_funcs=reward_with_length_penalty,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    # processing_class=train_tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd2cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SFT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c0e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"/mnt/bn/brench-lf-volume/wkq_wsp/train_result/Qwen3-4B-Inst-SFT-1113\",\n",
    "    chat_template_path=\"/mnt/bn/brench-lf-volume/wkq_wsp/models/Qwen3-4B-Instruct-2507\",\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"tensorboard\",\n",
    "    num_train_epochs=8,\n",
    "    logging_steps=2,\n",
    "    # 可选优化配置\n",
    "    gradient_checkpointing=True,  # 节省显存\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=\"/mnt/bn/brench-lf-volume/wkq_wsp/models/Qwen3-4B-Instruct-2507\",\n",
    "    args=sft_config,\n",
    "    train_dataset=load_dataset(\"json\", data_files=\"/opt/tiger/DataX/train/yzc-train-data-2.58k.jsonl\", split=\"train\"),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "inference\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 1. 加载训练好的模型\n",
    "model_path = \"/mnt/bn/brench-lf-volume/wkq_wsp/train_result/Qwen3-4B-Inst-SFT-1113/checkpoint-82\"  # 替换为具体checkpoint\n",
    "# 或者使用最终模型: \n",
    "# model_path = \"/mnt/bn/brench-lf-volume/wkq_wsp/train_result/Qwen3-4B-Inst-GRPO-1113\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,  # 与训练时一致\n",
    "    device_map=\"auto\"  # 自动分配设备\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# 2. 准备输入\n",
    "user_input = \"\"\"### 视频描述\n",
    "视频中，一位身穿蓝色衬衫的男子正在讲述一个街头发生的事件。画面中首先出现一位大爷摔倒的场景，一位小姐姐见状上前去扶大爷，然而大爷一把抓住小姐姐，坚称是小姐姐撞到了他。随后视频中不断插入街头现场的画面，展示了周围其他人的反应以及事件进一步的发展情况，包括现场不同人物的互动等，详细呈现了大爷摔倒后与小姐姐之间产生纠纷的整个过程，从小姐姐上前搀扶大爷，到大爷抓住小姐姐声称被撞，再到现场其他人的相关举动，完整地展现了这一具有争议性的街头事件的来龙去脉。\n",
    "\n",
    "### 视频分类\n",
    "时政社会\n",
    "\n",
    "\"\"\"  # 替换为实际输入\n",
    "\n",
    "# 构造ChatML格式的输入\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"你是一个发评机器人，针对输入的内容生成6条不同角度的评论内容，用换行符分隔。评论字数控制在12字左右。\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": user_input}\n",
    "]\n",
    "\n",
    "# 使用tokenizer的chat_template格式化\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# 3. Tokenize输入\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 4. 生成回复\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,  # 根据你的base_length=12调整\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# 5. 解码输出\n",
    "generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "# print(f\"用户输入: {user_input}\")\n",
    "print(f\"模型回复: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
